{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import nltk\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Dataset/dataset_fix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kamus_alay_1 = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/nasalsabila/kamus-alay/master/colloquial-indonesian-lexicon.csv\",\n",
    "    usecols=[\"slang\", \"formal\"])\n",
    "\n",
    "kamus_alay_2 = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/haryoa/indo-collex/main/dict/inforformal-formal-Indonesian-dictionary.tsv\",\n",
    "    sep=\"\\t\"\n",
    ")\n",
    "kamus_alay_2.columns=[\"slang\", \"formal\"]\n",
    "\n",
    "# generate kamus alay\n",
    "dict_alay = dict()\n",
    "for index, row in kamus_alay_1.iterrows():\n",
    "    dict_alay[row['slang']] = row['formal']\n",
    "for index, row in kamus_alay_2.iterrows():\n",
    "    dict_alay[row['slang']] = row['formal']\n",
    "    \n",
    "# price normalization regex\n",
    "prefix_harga = '(idr|rp)'\n",
    "suffix_harga = '(k|rb|rupiah|ribu|rban|an)'\n",
    "re_pattern = [\n",
    "    f'{prefix_harga}[\\d]+{suffix_harga}', # prefix + suffix\n",
    "    f'{prefix_harga}[\\d]+', # prefix\n",
    "    f'[\\d]+{suffix_harga}', # suffix\n",
    "    '\\d\\d\\d\\d[\\d]+' # digit minimal 5 / diatas 10,000\n",
    "]\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "    \n",
    "def pre_process_word(word, kamus_alay=dict_alay, regex_patterns=re_pattern, process_alay=False):\n",
    "    # BASIC PREPROCESSING \n",
    "    word = word.lower()                         # case folding\n",
    "    word = re.sub('[^\\w\\s]', '', word)         # remove punctuation\n",
    "    word = re.sub(r'[^\\x00-\\x7F]+', '', word)  # remove emoji\n",
    "    word = re.sub('[\\s]+', '', word)           # remove extra whitespace\n",
    "        \n",
    "    # remove duplicate consecutive char (max=2)\n",
    "#     word = re.sub(r'(.)(?=\\1\\1)', '', word)     \n",
    "    \n",
    "    # ubah kata berulang (minimal 3huruf diulang) cth : kemanamana -> kemana2\n",
    "#     re_search = re.search(r'(\\w+)(?=\\1)', word)\n",
    "#     if (re_search):\n",
    "#         start_idx = re_search.end()\n",
    "#         end_idx = re_search.end() + len(re_search.group())\n",
    "#         if (end_idx - start_idx) >= 3:\n",
    "#             word = word[:start_idx] + '2' + word[end_idx:]\n",
    "        \n",
    "    #  bahasa alay\n",
    "    if not process_alay:\n",
    "        if word in kamus_alay.keys():\n",
    "            word = kamus_alay[word]\n",
    "        \n",
    "    # stemming\n",
    "    \n",
    "    word = stemmer.stem(word)\n",
    "        \n",
    "    # normalisasi harga\n",
    "#     for pattern in regex_patterns:\n",
    "#         word = re.sub(pattern, '100k', str(word))\n",
    "        \n",
    "    return word\n",
    "\n",
    "# apply preprocess to alay dict\n",
    "processed_kamus_alay = dict()\n",
    "for key in dict_alay.keys():\n",
    "    new_key = pre_process_word(key, process_alay=True)\n",
    "    processed_kamus_alay[new_key] = dict_alay[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-FOOD': 1,\n",
       " 'I-FOOD': 2,\n",
       " 'B-MISCELLANEOUS': 3,\n",
       " 'I-MISCELLANEOUS': 4,\n",
       " 'B-SERVICE': 5,\n",
       " 'I-SERVICE': 6,\n",
       " 'B-AMBIENCE': 7,\n",
       " 'I-AMBIENCE': 8,\n",
       " 'B-PRICE': 9,\n",
       " 'I-PRICE': 10}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic={}\n",
    "for i, tag in enumerate(df.Tag.unique()):\n",
    "    dic[tag] = i\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[\"Tag\"].apply(lambda x:dic[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_kalimat = []\n",
    "for i in range(df[\"Kalimat #\"].min(),df[\"Kalimat #\"].max()+1):\n",
    "    list_kata = [\"<S>\"]\n",
    "    for kata in df[df[\"Kalimat #\"] == i][\"Word\"]:\n",
    "        list_kata.append(pre_process_word(str(kata), kamus_alay=processed_kamus_alay))\n",
    "    list_kata.append(\"</S>\")\n",
    "    list_kalimat.append(list_kata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_kalimat_join = []\n",
    "for i,kalimat in enumerate(list_kalimat):\n",
    "    list_kalimat_join.append(\" \".join(kalimat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<S> baru ke sini lagi telah beberapa lama absen kali ini saya coba matcha cheesecakenya dengan harga 315k nett paling mahal banding ukur slice di lain matchanya masuk yang manis bukan pahit begitu tapi lebih enak kok porsi lihat standard tapi nyata ngenyangin juga telah makan varian rasa ada chocolate caramel strawberry blueberry raspberry red velvet almond dan lain lupa hahaha cocok buat yang lagi ngidem cheesecake tapi tidak ken beli loyang </S>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_kalimat_join[808]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_review = list_kalimat_join[808]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = []\n",
    "for kalimat in (list_kalimat):\n",
    "    for i in range(len(kalimat)):\n",
    "        if i > 0 and i < len(kalimat)-1:\n",
    "            trigram.append([kalimat[i-1], kalimat[i], kalimat[i+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5918 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "NUM_WORDS=10000\n",
    "tokenizer = Tokenizer(num_words=10000,filters='!\"#$%&()*+,-.:;=?@[\\\\]^_`{|}~\\t\\n\\'',\n",
    "                      lower=True)\n",
    "tokenizer.fit_on_texts(trigram)\n",
    "sequences_train = tokenizer.texts_to_sequences(trigram)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "toEncode = labels.values.reshape(-1, 1)\n",
    "enc = enc.fit(toEncode)\n",
    "Encoded = enc.transform(toEncode).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trigram = pd.DataFrame(columns=[\"Trigram\", \"Label\", \"Trigram Encoded\", \"Label Encoded\"], data=zip(trigram, df[\"Tag\"].values, sequences_train, Encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trigram</th>\n",
       "      <th>Label</th>\n",
       "      <th>Trigram Encoded</th>\n",
       "      <th>Label Encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[&lt;S&gt;, di tiap, oleh sebab itu]</td>\n",
       "      <td>O</td>\n",
       "      <td>[19, 561, 30]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[di tiap, oleh sebab itu, ke sini]</td>\n",
       "      <td>O</td>\n",
       "      <td>[561, 30, 64]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[oleh sebab itu, ke sini, tidak]</td>\n",
       "      <td>O</td>\n",
       "      <td>[30, 64, 6]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Trigram Label Trigram Encoded  \\\n",
       "0      [<S>, di tiap, oleh sebab itu]     O   [19, 561, 30]   \n",
       "1  [di tiap, oleh sebab itu, ke sini]     O   [561, 30, 64]   \n",
       "2    [oleh sebab itu, ke sini, tidak]     O     [30, 64, 6]   \n",
       "\n",
       "                                       Label Encoded  \n",
       "0  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trigram.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_trigram[\"Trigram Encoded\"].values, \n",
    "    df_trigram[\"Label Encoded\"].values, \n",
    "    test_size=0.2, \n",
    "    random_state=50, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([np.array(x).astype('float32') for x in X_train])\n",
    "y_train = np.array([np.array(x).astype('float32') for x in y_train])\n",
    "\n",
    "X_test = np.array([np.array(x).astype('float32') for x in X_test])\n",
    "y_test = np.array([np.array(x).astype('float32') for x in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "general_200 = Word2Vec.load(\"embedding/idwiki_word2vec_200_new_lower.model\")\n",
    "word_vectors_general = general_200.wv\n",
    "\n",
    "known_words = 0\n",
    "unknown_words = 0\n",
    "\n",
    "GENERAL_EMBEDDING_DIM = 200\n",
    "vocabulary_size = min(len(word_index)+1,NUM_WORDS)\n",
    "embedding_matrix = np.zeros((vocabulary_size, GENERAL_EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i>=NUM_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors_general[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        known_words += 1\n",
    "    except KeyError:\n",
    "        embedding_matrix[i] = np.random.normal(0,np.sqrt(0.25),GENERAL_EMBEDDING_DIM)\n",
    "        unknown_words += 1\n",
    "\n",
    "del(word_vectors_general)\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "general_embedding_layer = Embedding(vocabulary_size,\n",
    "                            GENERAL_EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# pergikuliner_word2vec_100.model\n",
    "# domain_embedding/domain_specific_skip_word2vec_100_10_5.model\n",
    "# domain_embedding/domain_specific_skip_word2vec_100_15_10_basic_alay.model\n",
    "# domain_embedding/domain_specific_skip_word2vec_100_15_10_basic_alay_normalize.model\n",
    "# domain_embedding/domain_specific_skip_word2vec_100_15_10_basic_alay_stem.model\n",
    "domain_100 = Word2Vec.load(\"domain_embedding/domain_specific_skip_word2vec_100_15_10_basic_alay_stem.model\")\n",
    "word_vectors_domain = domain_100.wv\n",
    "\n",
    "known_words = []\n",
    "unknown_words = []\n",
    "\n",
    "DOMAIN_EMBEDDING_DIM = 100\n",
    "vocabulary_size = min(len(word_index)+1,NUM_WORDS)\n",
    "embedding_matrix = np.zeros((vocabulary_size, DOMAIN_EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i>=NUM_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors_domain[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        known_words.append(word)\n",
    "    except KeyError:\n",
    "        embedding_matrix[i] = np.random.normal(0,np.sqrt(0.25),DOMAIN_EMBEDDING_DIM)\n",
    "        unknown_words.append(word)\n",
    "\n",
    "domain_embedding_layer = Embedding(vocabulary_size,\n",
    "                            DOMAIN_EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 3)]               0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 3, 200)            1183800   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 3, 256)           336896    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " average_pooling1d (AverageP  (None, 1, 256)           0         \n",
      " ooling1D)                                                       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 11)                2827      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,523,523\n",
      "Trainable params: 1,523,523\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import SpatialDropout1D, Input, Dense, GRU, Embedding, Dropout, LSTM, concatenate,Bidirectional, AveragePooling1D, Reshape, Flatten\n",
    "# from tensorflow.keras.layers.core import Reshape, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "sequence_length = X_train.shape[1]\n",
    "num_filters_blstm = 128\n",
    "num_filters_lstm = 128\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "\n",
    "embedding = general_embedding_layer(inputs)\n",
    "# embedding = domain_embedding_layer(inputs)\n",
    "\n",
    "bilstm = Bidirectional(LSTM(num_filters_blstm, \n",
    "                           dropout=0.5, recurrent_dropout=0.3, \n",
    "                        return_sequences=True))(embedding)\n",
    "lstm = LSTM(num_filters_lstm, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(embedding)\n",
    "\n",
    "AvPool = AveragePooling1D(pool_size=(2), strides=None)(bilstm)\n",
    "flatten = Flatten()(AvPool)\n",
    "\n",
    "output = Dense(units=11, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(flatten)\n",
    "\n",
    "model = Model(inputs, output)\n",
    "\n",
    "adam = Adam(learning_rate=1e-3)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['acc'])\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=3, verbose=1,\n",
    "                          restore_best_weights=True\n",
    "                          )]\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "316/316 [==============================] - 12s 26ms/step - loss: 1.1562 - acc: 0.7031 - val_loss: 0.9570 - val_acc: 0.7238\n",
      "Epoch 2/100\n",
      "316/316 [==============================] - 9s 27ms/step - loss: 0.9345 - acc: 0.7319 - val_loss: 0.8756 - val_acc: 0.7412\n",
      "Epoch 3/100\n",
      "316/316 [==============================] - 9s 27ms/step - loss: 0.8573 - acc: 0.7482 - val_loss: 0.8269 - val_acc: 0.7521\n",
      "Epoch 4/100\n",
      "316/316 [==============================] - 9s 28ms/step - loss: 0.8085 - acc: 0.7592 - val_loss: 0.7831 - val_acc: 0.7628\n",
      "Epoch 5/100\n",
      "316/316 [==============================] - 9s 28ms/step - loss: 0.7655 - acc: 0.7691 - val_loss: 0.7626 - val_acc: 0.7645\n",
      "Epoch 6/100\n",
      "316/316 [==============================] - 9s 28ms/step - loss: 0.7307 - acc: 0.7780 - val_loss: 0.7430 - val_acc: 0.7704\n",
      "Epoch 7/100\n",
      "316/316 [==============================] - 9s 29ms/step - loss: 0.7058 - acc: 0.7839 - val_loss: 0.7301 - val_acc: 0.7742\n",
      "Epoch 8/100\n",
      "316/316 [==============================] - 11s 34ms/step - loss: 0.6845 - acc: 0.7894 - val_loss: 0.7260 - val_acc: 0.7771\n",
      "Epoch 9/100\n",
      "316/316 [==============================] - 14s 43ms/step - loss: 0.6631 - acc: 0.7963 - val_loss: 0.7143 - val_acc: 0.7785\n",
      "Epoch 10/100\n",
      "316/316 [==============================] - 12s 37ms/step - loss: 0.6488 - acc: 0.7998 - val_loss: 0.7057 - val_acc: 0.7842\n",
      "Epoch 11/100\n",
      "316/316 [==============================] - 13s 41ms/step - loss: 0.6369 - acc: 0.8018 - val_loss: 0.7015 - val_acc: 0.7848\n",
      "Epoch 12/100\n",
      "316/316 [==============================] - 12s 36ms/step - loss: 0.6249 - acc: 0.8065 - val_loss: 0.6988 - val_acc: 0.7868\n",
      "Epoch 13/100\n",
      "316/316 [==============================] - 12s 39ms/step - loss: 0.6127 - acc: 0.8103 - val_loss: 0.6953 - val_acc: 0.7874\n",
      "Epoch 14/100\n",
      "316/316 [==============================] - 11s 35ms/step - loss: 0.6029 - acc: 0.8134 - val_loss: 0.6955 - val_acc: 0.7884\n",
      "Epoch 15/100\n",
      "316/316 [==============================] - 12s 37ms/step - loss: 0.5942 - acc: 0.8168 - val_loss: 0.6924 - val_acc: 0.7877\n",
      "Epoch 16/100\n",
      "316/316 [==============================] - 12s 39ms/step - loss: 0.5843 - acc: 0.8186 - val_loss: 0.6910 - val_acc: 0.7879\n",
      "Epoch 17/100\n",
      "316/316 [==============================] - 12s 37ms/step - loss: 0.5761 - acc: 0.8221 - val_loss: 0.6906 - val_acc: 0.7850\n",
      "Epoch 18/100\n",
      "316/316 [==============================] - 12s 38ms/step - loss: 0.5737 - acc: 0.8215 - val_loss: 0.6915 - val_acc: 0.7893\n",
      "Epoch 19/100\n",
      "316/316 [==============================] - 12s 38ms/step - loss: 0.5645 - acc: 0.8258 - val_loss: 0.6875 - val_acc: 0.7903\n",
      "Epoch 20/100\n",
      "316/316 [==============================] - 12s 37ms/step - loss: 0.5575 - acc: 0.8278 - val_loss: 0.6910 - val_acc: 0.7905\n",
      "Epoch 21/100\n",
      "316/316 [==============================] - 13s 40ms/step - loss: 0.5513 - acc: 0.8302 - val_loss: 0.6848 - val_acc: 0.7892\n",
      "Epoch 22/100\n",
      "316/316 [==============================] - 11s 35ms/step - loss: 0.5476 - acc: 0.8316 - val_loss: 0.6938 - val_acc: 0.7857\n",
      "Epoch 23/100\n",
      "316/316 [==============================] - 12s 36ms/step - loss: 0.5447 - acc: 0.8327 - val_loss: 0.6873 - val_acc: 0.7916\n",
      "Epoch 24/100\n",
      "315/316 [============================>.] - ETA: 0s - loss: 0.5364 - acc: 0.8360Restoring model weights from the end of the best epoch: 21.\n",
      "316/316 [==============================] - 12s 38ms/step - loss: 0.5364 - acc: 0.8360 - val_loss: 0.6941 - val_acc: 0.7913\n",
      "Epoch 00024: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcb6b7e99a0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, \n",
    "          batch_size=128, \n",
    "          epochs=100, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test, y_test),\n",
    "          callbacks=callbacks\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_argmax = [np.argmax(i) for i in y_test]\n",
    "y_pred_argmax = [np.argmax(i) for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.571     0.361     0.442       413\n",
      "           2      0.656     0.587     0.619      1349\n",
      "           3      0.324     0.101     0.154       109\n",
      "           4      0.500     0.276     0.356       398\n",
      "           5      0.676     0.373     0.481        67\n",
      "           6      0.673     0.504     0.576       282\n",
      "           7      0.750     0.411     0.531        73\n",
      "           8      0.619     0.551     0.583       236\n",
      "           9      0.929     0.289     0.441        45\n",
      "          10      0.670     0.520     0.586       125\n",
      "\n",
      "   micro avg      0.629     0.474     0.540      3097\n",
      "   macro avg      0.637     0.397     0.477      3097\n",
      "weighted avg      0.619     0.474     0.530      3097\n",
      "\n",
      "f1 score: 0.7891768895924021\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, make_scorer, f1_score\n",
    "\n",
    "print(classification_report(\n",
    "    y_test_argmax, y_pred_argmax,\n",
    "    labels=[1,2,3,4,5,6,7,8,9,10],\n",
    "    digits=3))\n",
    "print(\"f1 score:\", f1_score(y_test_argmax, y_pred_argmax, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
